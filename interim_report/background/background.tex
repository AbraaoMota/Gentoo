\chapter{Background}
Types of scanning - network based vs host based scanning


\section {Website Vulnerabilities}

This project is rooted in identifying and mitigating web application vulnerabilities. In order to do so, it is essential to understand what these are, their impact, and what steps are necessary to prevent them. Fortunately, there are great wealths of information available to learn more about vulnerabilities. A great starting point is \textit{OWASP}, the Open Web Application Security Project. This is a community driven effort into improving the safety of software across the world, and the organisation has taken extensive steps into creating useful guides for developers wishing to know more. A particularly convenient resource they provide is a consensus of the top 10 security risks that web applications face today. \\

At the time of writing, this list contains the following risks, some of which are appropriate to pursue in this project: \\

	\emph{Injection} - This risk arises from any place on a website that accepts client controlled input. This may be through the more obvious - submission of web forms and search boxes, but also includes more subtle ways of the user providing input, such as URL or HTTP request parameters. Accepting this input per se is not a vulnerability, but the issue lies in blindly trusting this input to not be malicious. Whenever this input is used to query a database or perform server-side commands, if it has not been sanitized (cleaned to delete potentially dangerous input combinations), it has potential to leak or permanently corrupt information for the website owner. Due to its prevalence and modus operandi, this is an appropriate vulnerability to scan for and detect in this project. \\
	
	\emph{Broken Authentication} - This can encompass a wide range of things, such as use of weak or default passwords and admin accounts, or poor management of session identifiers such that these can be easily manipulated. It can also include flawed password recovery mechanisms. These risks could be analysed as part of the security analysis ran by the tool, and with support from user input could be combined with scans to effectively detect weak authentication mechanisms. \\
	
	\emph{Sensitive Data Exposure} - This risk is a result of using weak or poorly protected cryptographic measures. If a website has left their encryption keys in plaintext for someone to be able to find, or doesn't use HTTPS altogether, it may be exposed to this attack vector. The tool in question could look for a lack of TLS enforcement across pages, or attempt to force a connection downgrade (from HTTPS to HTTP) to guide the user in the right direction of finding sensitive information. \\
	
	\emph{XML External Entities} - This vulnerability exists in applications that accept or include XML data from a 3rd party. A malicious user could use this data format to attempt to exfiltrate sensitive data from the handling server. Identifying this risk without user input may prove to be difficult, but could be within the potential vulnerabilities considered by the tool. \\
	
	\emph{Broken Access Control} - This risk is comprised of all the possible ways in which an application might allow a user to perform actions that should be restricted to their access level (for example, allowing a non-admin user to read bank balances of arbitrary accounts in the system). Determining what is and isn't an allowed action on a website varies tremendously per application, so this is a very difficult task to automate, and isn't ideal to try and incorporate as part of the final tool.   \\
	
	\emph{Security Misconfiguration} - A poorly setup server may suffer from this risk if there are components or services installed by default that are not prepared accordingly to the necessary security measures - such as disabling error stack traces from services; these may reveal more than what a website owner thinks when in the context of being attacked. This vulnerability type is well suited for automatic scans that scour the website for versioning details of services being ran, which may in turn reveal known weaknesses to look for in the case of a negligent set up.\\
	
	\emph{Cross-Site Scripting (XSS)} - One of the most well known risks for web developers today is XSS - this is exploited when a user successfully injects Javascript into a website, causing a non-intended script to run. This is a severe risk depending on how many users it might affect, it can range from Self XSS which affects only the user injecting this content, but can also be seen as Stored XSS, whereby a malicious script is stored in a database, and can be potentially retrieved and ran by other users, posing serious risks where credentials and other session information can be stolen. This lends itself well to the purpose of the application to be developed in this project, as it can test a wide variety of known inputs to expose this vulnerability. \\
	
	\emph{Insecure Deserialisation} - Serialisation is the process of converting a data structure into a format that can be easily transferred over a connection. This resulting new format must then be deserialised to obtain the initial information back. An attacker could craft a serialized object such that it exploits the process or properties of deserialisation in the target application to obtain access to privileged data. This process will vary depending on the application domain and intended data structures, so it is not ideal for automated tools to attempt to tackle this issue. \\ 
	
	\emph{Using components with known vulnerabilities} - In application architectures that heavily rely on a variety of components or libraries from different sources, it can often be hard to ensure that these are all kept up to date. In instances where they are not, it becomes a simple task for a scanner to produce a map of outdated version numbers to possible exploits that have been discovered on the component since then. Once a CVE (Common vulnerabilities and Exposures - a unique identification method for security vulnerabilities found worldwide) is produced as a result from a scan, it is just a matter of reproducing exploit steps found online to endanger the application. \\ 
	
	\emph{Insufficient Logging and Monitoring} - An ideal web application keeps a track of all activities and accesses that occur. This helps provide accountability for actions. When this is not the case, it weakens the position of the website administrators to pinpoint attack culprits. This is very hard to detect from an outsider perspective, and \textit{insufficient} is an objective term; it wouldn't be fruitful to include this in an externally ran vulnerability scanner. \\


\section{Vulnerability Scanners}

\subsection{Black Box}

\subsection{Automated scanning}
This project is not the first piece of work dedicated to solving this problem using 

 
 
 Human judgement is needed anyway - scanner won't know for sure. It's a human process to hack, and people will spend hours in crafting specialised attacks. 
\section{Browser extensions}
 
 
 
 
 

 
\section{Limitations}

\subsection{Human review}

 \subsection{Breadth of  work}
 
 \subsection{Self security}
 	 My own extension is not free from attacks / bugs
http://slideplayer.com/slide/4352044/

\subsection{Ethics \& Handling of Results}

scans might take the website down if intensive
 
