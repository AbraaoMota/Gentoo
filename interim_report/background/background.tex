\chapter{Background}
Types of scanning - network based vs host based scanning


\section {Website Vulnerabilities}

This project is rooted in identifying and mitigating web application vulnerabilities. In order to do so, it is essential to understand what these are, their impact, and what steps are necessary to prevent them. Fortunately, there are great wealths of information available to learn more about vulnerabilities. A great starting point is \textit{OWASP}, the Open Web Application Security Project. This is a community driven effort into improving the safety of software across the world, and the organisation has taken extensive steps into creating useful guides for developers wishing to know more. A particularly convenient resource they provide is a consensus of the top 10 security risks that web applications face today. \\

At the time of writing, this list contains the following risks, some of which are appropriate to pursue in this project: \\

	\emph{Injection} - This risk arises from any place on a website that accepts client controlled input. This may be through the more obvious - submission of web forms and search boxes, but also includes more subtle ways of the user providing input, such as URL or HTTP request parameters. Accepting this input per se is not a vulnerability, but the issue lies in blindly trusting this input to not be malicious. Whenever this input is used to query a database or perform server-side commands, if it has not been sanitized (cleaned to delete potentially dangerous input combinations), it has potential to leak or permanently corrupt information for the website owner. Due to its prevalence and modus operandi, this is an appropriate vulnerability to scan for and detect in this project. \\
	
	\emph{Broken Authentication} - This can encompass a wide range of things, such as use of weak or default passwords and admin accounts, or poor management of session identifiers such that these can be easily manipulated. It can also include flawed password recovery mechanisms. These risks could be analysed as part of the security analysis ran by the tool, and with support from user input could be combined with scans to effectively detect weak authentication mechanisms. \\
	
	\emph{Sensitive Data Exposure} - This risk is a result of using weak or poorly protected cryptographic measures. If a website has left their encryption keys in plaintext for someone to be able to find, or doesn't use HTTPS altogether, it may be exposed to this attack vector. The tool in question could look for a lack of TLS enforcement across pages, or attempt to force a connection downgrade (from HTTPS to HTTP) to guide the user in the right direction of finding sensitive information. \\
	
	\emph{XML External Entities} - This vulnerability exists in applications that accept or include XML data from a 3rd party. A malicious user could use this data format to attempt to exfiltrate sensitive data from the handling server. Identifying this risk without user input may prove to be difficult, but could be within the potential vulnerabilities considered by the tool. \\
	
	\emph{Broken Access Control} - This risk is comprised of all the possible ways in which an application might allow a user to perform actions that should be restricted to their access level (for example, allowing a non-admin user to read bank balances of arbitrary accounts in the system). Determining what is and isn't an allowed action on a website varies tremendously per application, so this is a very difficult task to automate, and isn't ideal to try and incorporate as part of the final tool.   \\
	
	\emph{Security Misconfiguration} - A poorly setup server may suffer from this risk if there are components or services installed by default that are not prepared accordingly to the necessary security measures - such as disabling error stack traces from services; these may reveal more than what a website owner thinks when in the context of being attacked. This vulnerability type is well suited for automatic scans that scour the website for versioning details of services being ran, which may in turn reveal known weaknesses to look for in the case of a negligent set up.\\
	
	\emph{Cross-Site Scripting (XSS)} - One of the most well known risks for web developers today is XSS - this is exploited when a user successfully injects Javascript into a website, causing a non-intended script to run. This is a severe risk depending on how many users it might affect, it can range from Self XSS which affects only the user injecting this content, but can also be seen as Stored XSS, whereby a malicious script is stored in a database, and can be potentially retrieved and ran by other users, posing serious risks where credentials and other session information can be stolen. This lends itself well to the purpose of the application to be developed in this project, as it can test a wide variety of known inputs to expose this vulnerability. \\
	
	\emph{Insecure Deserialisation} - Serialisation is the process of converting a data structure into a format that can be easily transferred over a connection. This resulting new format must then be deserialised to obtain the initial information back. An attacker could craft a serialized object such that it exploits the process or properties of deserialisation in the target application to obtain access to privileged data. This process will vary depending on the application domain and intended data structures, so it is not ideal for automated tools to attempt to tackle this issue. \\ 
	
	\emph{Using components with known vulnerabilities} - In application architectures that heavily rely on a variety of components or libraries from different sources, it can often be hard to ensure that these are all kept up to date. In instances where they are not, it becomes a simple task for a scanner to produce a map of outdated version numbers to possible exploits that have been discovered on the component since then. Once a CVE (Common vulnerabilities and Exposures - a unique identification method for security vulnerabilities found worldwide) is produced as a result from a scan, it is just a matter of reproducing exploit steps found online to endanger the application. \\ 
	
	\emph{Insufficient Logging and Monitoring} - An ideal web application keeps a track of all activities and accesses that occur. This helps provide accountability for actions. When this is not the case, it weakens the position of the website administrators to pinpoint attack culprits. This is very hard to detect from an outsider perspective, and \textit{insufficient} is an objective term; it wouldn't be fruitful to include this in an externally ran vulnerability scanner. \\

	This is not an exhaustive list, but is useful to consider when attempting to address specific aspects for development in the tool. There are several other features or protocols that can be checked to better ascertain the security of a website, such as:
\begin{multicols}{3}
	\begin{itemize}
	\item   Use of iFrames
	\item 	Cookies
	\item 	Appropriate headers
	\item 	CSRF mitigations
	\item 	Use of HTTPS
	\item   HSTS
	\item 	JS instrumentation
	\item 	Remote inclusion and dependency analysis
	\item 	DoS
	\item 	Clickjacking
	\item 	CSP and sandboxing policies
	\end{itemize}
\end{multicols}



\section{Vulnerability Scanners}

This project is not the first piece of work aimed at detecting website vulnerabilities. There have been efforts in academia to address this issue \textbf{ADD REFERENCES HERE}. There are also other commercial and open source tools designed to solve this problem such as \textit{Acunetix} \textbf{REFERENCE}, IBM's \textit{AppScan} and \textit{w3af}. In order to appreciate the aspects in which this project aims to innovate in, it is fruitful to understand some aspects used to refer to these scanners.

\subsection{Black Box}

Much of the reading in this area makes reference to \textit{black box} scanners. This term is simply used for the tools that analyse a website without access to its source code. The opposite of this would be a \textit{white box} scanner - this sort of program works through static analysis on the raw code of the web application. A major differentiation between the two approaches is their use case; a white box scanner emulates the experience of a web developer who is looking to detect potentially exposed parts of his code, whereas a black box scanner portrays the more common setting for a malicious attacker, whereby they are only working with the exterior interface of the application to forge an intrusion. The black box approach is the one I will be using in the extension implementation as it aligns best with the project requisites.

\subsection{Automated scanning}
 
 
 Human judgement is needed anyway - scanner won't know for sure. It's a human process to hack, and people will spend hours in crafting specialised attacks. 
 
 "If you can’t crawl it you can’t scan it! " - Acunetix - Commercial tool - https://www.acunetix.com/vulnerability-scanner/javascript-html5-security/
 
 
 "C) Adding interactive multistep options to scan. Out of the three scanners, only one scanner had that feature. While the option to set login credentials helps to provide application’s login info for the scanning, it does not instruct the scanners how to visit the pages and where to put attack vectors to exploit and to detect the vulnerabilities." - Analysis of effectivness of black box web app scanners 
 
 
 "Our experimental findings confirmed that choosing the right attack vectors for the detection and exploitation of stored XSS and stored SQLI remains a big challenge for black box scanners." - Analysis of effectiveness of black box web application scanners in detection of stored sql injection and stored xss vulnerabilities (2015)
 
 "Scanners need to be improved to use right attack vectors in the right situations" - Analysis of effectiveness of black box web application scanners in detection of stored sql injection and stored xss vulnerabilities (2015)
 
\section{Browser extensions}
 
 
 
 
 

 
\section{Limitations}

\subsection{Human review}

 \subsection{Breadth of  work}
 
 \subsection{Self security}
 	 My own extension is not free from attacks / bugs
http://slideplayer.com/slide/4352044/

\subsection{Ethics \& Handling of Results}

scans might take the website down if intensive
 
